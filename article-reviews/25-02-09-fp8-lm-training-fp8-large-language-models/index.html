<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>FP8-LM - Training FP8 Large Language Models | Senan Adonis Kassem</title>
<meta name="keywords" content="NLP, Transformers, Deep Learning, LLM&#39;s">
<meta name="description" content="FP8 mixed-precision allows for dramatically lower computational costs and memory overheads while maintaining predictive performance. A must read!">
<meta name="author" content="">
<link rel="canonical" href="https://senankassem.com/article-reviews/25-02-09-fp8-lm-training-fp8-large-language-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f29b4ec3ee6e98a9ec1a9a51adb5b1bf468ca1f94fcea7e42282ee958b7e23e5.css" integrity="sha256-8ptOw&#43;5umKnsGppRrbWxv0aMoflPzqfkIoLulYt&#43;I&#43;U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://senankassem.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://senankassem.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://senankassem.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://senankassem.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://senankassem.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://senankassem.com/article-reviews/25-02-09-fp8-lm-training-fp8-large-language-models/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://senankassem.com/article-reviews/25-02-09-fp8-lm-training-fp8-large-language-models/">
  <meta property="og:site_name" content="Senan Adonis Kassem">
  <meta property="og:title" content="FP8-LM - Training FP8 Large Language Models">
  <meta property="og:description" content="FP8 mixed-precision allows for dramatically lower computational costs and memory overheads while maintaining predictive performance. A must read!">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="article-reviews">
    <meta property="article:published_time" content="2025-02-09T21:08:00+00:00">
    <meta property="article:modified_time" content="2025-02-09T21:08:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FP8-LM - Training FP8 Large Language Models">
<meta name="twitter:description" content="FP8 mixed-precision allows for dramatically lower computational costs and memory overheads while maintaining predictive performance. A must read!">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Article Reviews",
      "item": "https://senankassem.com/article-reviews/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "FP8-LM - Training FP8 Large Language Models",
      "item": "https://senankassem.com/article-reviews/25-02-09-fp8-lm-training-fp8-large-language-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "FP8-LM - Training FP8 Large Language Models",
  "name": "FP8-LM - Training FP8 Large Language Models",
  "description": "FP8 mixed-precision allows for dramatically lower computational costs and memory overheads while maintaining predictive performance. A must read!",
  "keywords": [
    "NLP", "Transformers", "Deep Learning", "LLM's"
  ],
  "articleBody": "Introduction Training Large Language Models (LLMs) is slow and costly; they are computationally expensive, requiring a lot of compute power, memory allocation, and time to effectively train. The current paradigm of training LLMs uses 16-bit number representations (BF16), 32-bit number representations (FP32), or a mixture of both.\nThe reason BF16 has been used as opposed to FP16 is because BF16 offers a wider range of potential values at the expense of precision, due to its exponent occupying more of the bits rather than the mantissa, which prevents underflow and overflow in comparison to FP16. This leads to better numerical stability while matching the performance of the full-precision FP32 using half the number of bits leading to a reduction in its memory footprints, improving computational efficiency and reducing communication overhead.\nSo although higher-precision number representations have the capability of representing numbers more precisely at the expense of extra space in memory and higher operational costs for mathematical operations (this may be worth the tradeoff in particular domains of scientific research, engineering simulations, and financial modeling where small uncertainties may compound leading to inaccurate calculations and therefore inaccurate conclusions), in the domain of deep learning the computational overhead is high. This means models must find a balance between the accuracy of high-precision floating point representations of numbers, and lower-precision representations that speed up and increase the efficiency of computations.\nThe current paradigm of deep learning emphasises increasing compute power to increase computational speeds of higher-precision numbers at the expense of increased compute costs. Additionally, it almost mandates that to improve the predictive capacity of neural networks, the only step forward is more data and compute power. This has set a precedent of maintaining higher-precision representations within hardware without offering true lower-precision representational capacities i.e. even if the GPUs/TPUs/CPUs offer FP8 support, their fundamental architectures are optimised for BF16, offering only minor perceived improvements in computational efficiency and speed when using FP8.\nThis paper theorizes that using a lower-precision number representation may lead to reductions in cost because they have lower communication overheads, faster numerical operations, and smaller memory footprints without significant deteriorations in overall predictive accuracy, which challenges the current approach of needing increased compute power for progression in the field of deep learning. It also argues that this change in number representation does not require changes to hyperparameters during training to maintain accuracy, signifying a significant (yet simple) improvement in the efficiency of training deep neural networks. And finally, it argues that these improvements are not linear, but rather scale multiplicatively as the size of the neural architecture (which has an exponentially increasing number of weights).\nKey Contributions \u0026 Insights This paper defines a novel approach FP8 mixed-precision training framework that incorporates 8-bit weights and gradients, 8-bit optimizers, and 8-bit distributed parallel training to allow for faster and more memory-efficient training while maintaining predictive accuracy of the models. This implementation does not require changes to hyper-parameters and training receipts. It addresses issues such as data underflow or overflow and quantization errors, which cause numerical instability and irreversible divergences throughout training, by using two techniques precision decoupling and automatic scaling. By using FP8 instead of BF16, they have noted a 39% reduction in real memory usage and 75% faster training.\nAutomatic Scaling Training LLMs with reduced-precision FP8 has its challenges; data overflow and underflow are much more problematic as the dynamic range and representation precision of FP8 is much lower than that of FP16 and BF16, which leads to training collapses caused by loss spikes (or even NaNs) and vanishing gradients. To address this, tensor scaling techniques are proposed - this is the idea where we multiply higher precision values with a scaling factor before converting them to FP8 in order to preserve the number in a range that corresponds with the representable range of FP8. This scaling factor is determined dynamically by checking the running statistics of the gradients and weights within the layers. This allows for higher-precision numbers to preserve their gradient values within the representation range when converted to FP8, alleviating underflow and overflow occurrences.\nFP8 Gradient and All-Reduce Communication\nThe typical method of storing gradients for computation is done in 16/32-bit datatypes, which results in high bandwidth requirements for collective communication during training. This is when parameters (such as gradients, model weights, and activations) need to be exchanged between GPUs, usually through some collective communication operation such as AllReduce (summing gradients across multiple GPUs), Broadcasting (sending updated weights to all GPUs), and AllGather (Sharing activations across GPUs for specific architectures). The higher the size of the datatype - i.e. the number of bits it takes in memory - the more bytes are required to be communicated for each number representation. For example, if a model had 100 parameters, storing and communicating in FP32 would require 4*100 bytes, while FP16/BF16 cuts this cost in half.\nAs a model size increases, the number of parameters may grow exponentially, leading to exponentially increasing communication overhead. Additionally, communication overhead also depends on the number of GPUs, which means that although GPUs help distribute computation, it also increases synchronization costs between the GPUs. These combined may lead to a non-trivial growing bottleneck during training as the complexity of the model increases, and the number of parallel batches are processed.\nDirectly applying FP8 to gradients leads to a decrease in accuracy because of the underflow and overflow problems arising from the low-bit AllReduce operation. The AllReduce operation typically aggregates gradients across GPUs using pre-scaling (dividing gradients before summation) and post-scaling (dividing after summation) within a batch of inputs, where the goal is to average the gradient tensor at a given $ i $th layer, denoted as $ g_i $, across different GPUs. But using the pre-scaling and post-scaling leads to an issue of underflowing and overflowing respectively, so this paper proposes automatic scaling as a method to address this issue.\nAdditionally, FP8 needs per-tensor scaling factors, but current GPU communication frameworks do not efficiently support reducing them across GPUs. Therefore, the scalar that is defined is calculated as a single global scaler that is shared across all GPUs, which ensures that all gradient tensors for a given $i$th layer, $g_i$, use the same shared scaling factor when quantized into FP8 format. This approach significantly reduces communication overhead by limiting the number of scalers transmitted, making synchronization steps highly efficient and allowing low-bit gradient communication without extra complexity\nPrecision Decoupling It has been shown that reducing the precision of an optimisers variables leads to accuracy degradation, which raises the question: which variables in the optimizer must be kept at high precision? This is where precision decoupling comes, where the goal is to decouple the influence of data precision on the variables in the optimizer and investigate which one can be assigned lower precision.\nFP8 Optimizers\nTraditional optimizers, such as Adam, maintain copies of their model weights, gradients, first-order and second-order gradient moments in 32-bit float format for numerical stability. This leads to large 16byte overhead per parameter during training. To try and reduce this, a mixed-precision approach is taken: Gradient statistics can use lower precision, while the master weights necessitate high precision; direction of the gradient holds greater significance than its magnitude.\nMaster weights need higher precision because during optimization weight updates can become extremely small or large making it harder to decipher the correct direction at low precision; the higher the precision helps prevent loss of information when updating weights, ensuring stable and accurate training in the correct direction. The first- and second-order gradient moments are used more to scale magnitude than direction so do not require as high precision The first-order gradient moment can tolerate a high quantization error and can be assigned with low-precision, albeit with some degradation. The second-order moment requires a higher precision than the first-order gradient moment because calculating the square of gradients for the second-order gradient moment might lead to data underflow due to the typically small gradient values. Adapting to FP8 to Different Distributed Parallel Training Distributed strategies are necessary for parallelizing training across multiple GPUs and are often used in a complementary way to increase parallelism and efficient training. These methods include\nData parallelism: Distributing the same model across multiple devices, each processing a different subset of the data simultaneously.\nSince this approach doesn‚Äôt involve splitting individual tensors or layers, integrating FP8 precision doesn‚Äôt require specific modifications. Pipeline parallelism: Dividing the model into sequential stages, each assigned to a different device, allowing different batches of data to be processed concurrently through the pipeline.\nSimilar to data parallelism, this method doesn‚Äôt necessitate additional FP8-specific adjustments because it doesn‚Äôt involve partitioning tensors within layers and independently allocates distinct layers to different portions of the GPU. Although it does not need to directly find a method to implement FP8, it is essential to ensure data integrity and congruency between the layers Tensor parallelism: Splitting individual tensors (such as weight matrices) across multiple devices to perform computations like matrix multiplications in parallel.\nThis type of distribution requires the sharded weights and activation tensors to be converted into FP8 format for computations in linear layers. This enables both forward computations and backward gradient communications to utilize FP8 precision, enhancing efficiency. Sequence parallelism: Distribute subsequences of the initial input into a transformer models across multiple devices, enabling parallel processing of different parts of the input sequence.\nThis type of distribution requires a converter, $g$, to manage the transition between sequence and tensor parallel regions. During the forward pass, an all-gather operation collects sequence partitions, while in the backward pass, a reduce-scatter operation handles tensor segments. Incorporating FP8 datatype conversion before these operations reduces communication costs across GPUs by transmitting low-bit FP8 activations. Zero Redundancy Optimizer (ZeRO): Memory optimization technique that builds on data parallelism\nPartitions the model states (parameters, gradients, and optimizer states) across multiple devices reducing memory redundancies Applying FP8 to this method is difficult because of managing scaling factors associated with FP8 Solution: Allocate each tensor as a whole across devices rather than partitioning into sub-tensors. This ensures that tensor scaling factors are distributed along with the tensor and reduces communication and computational complexity. Results Using FP8 mixed-precision did not lead to significant differences in the loss curves and demonstrate that the proposed FP8 mixed-precision scheme can achieve equivalent performance to those of its higher-precision counterparts across a diverse array of model scales, additionally there is comparable zero-short performance in comparison to downstream tasks in comparison to BF16 model counterparts. Furthermore, the communication costs and memory footprints dramatically decreased in comparison to its higher-precision counter parts, which further validates that models pre-trained with FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities comparable to those of higher-precision while lowering computational cost, communication overhead and reducing memory footprint.\nThese results also extrapolate to different approaches to LLMs, such as fine-tuning and reinforcement learning from human feedback, leading to dramatic decreases in memory utilization in the RL scenario (32% concerning model weights and 62% concerning optimizer states). This further reinforces the versatility and innovative solution FP8 has in advancing and improving the training process of LLMs while maintaining predictive quality.\nEvaluation of the Paper This paper introduced a groudbreaking, paradigm shifting FP8 mixed-precision training framework that significantly improves memory efficieny and training speeds without significant sacrifices in performance. It provides a comprehensive open-source implementation of FP8 accross a variety of training components - gradients, optimizers, distributed communication, and parallel training strategies - as well as provides performance benchmarks in a variety of different training paradigms - pretraining, fine-tuning and even reinforcement learning with human feedback tasks - differentiating itself from prior approaches and providing a general solution to the field of LLM‚Äôs as a whole.\nWith the results from this paper, it offers a way to make highly scalable solution for multi-GPU distributed training and opens the door to training even larger models at a lower computational cost and a faster pace. Its numerical stability methods of using automatic scaling rather than pre- and post-scaling ensures that FP8 weights and gradients are effectively trained through their corresponding optimizers at scale, and the decoupling of precision to ensure as much of the parameters are represented in FP8 increasing efficiency while preventing significant degradations in the model‚Äôs performance.\nThese results alone are paradigm-shifting.\nAdditionally, the paper has done all this with the current limitations of GPU communication and optimization for FP8 operations, demonstrating that these results are yet to see their peak advancements in the domain of training and optimizing a transformer based architectures.\nThe paper has also led to the more recent explosion of the DeepSeek model that utilizes FP8 mixed-precision in its training (along with some other innovative approaches, see DeepSeek-R1) in building a competent model with Chain-of-Thought (CoT) reasoning at a fraction of the cost of current large-scale LLMs. It provides the groundwork for advancements in the field and opens up the door for future research related to the field of effective computation and communication within and between GPUs, the effects of scaling up FP8 models (example), as well as in regards to our understanding of knowledge representations and updating the vector embeddings.\nNow comes some more critical analysis of this ground-breaking paper:\nFP8 Training may not be as robust for all architectures. The paper only focuses on GPT-style models (7B to 175B parameters); Different architectures, such as diffusion models, may exhibit different numerical stability issues with FP8 and the methods may not generalize well to non-transformer architectures. However, Mixture-of-Experts (MoE) - implemented by DeepSeek‚Äôs R1 model - has been shown to work with this FP8 mixed-precision training paradigm, so there may be more generalizability than less.\nThere is a lack of evaluation on long-context or novel tasks. While FP8 achieves comparable pretraining loss and zero-shot performance, there is no detailed analysis of fine-grained accuracy trade-offs in tasks that require long-term dependencies (e.g., long-context reasoning tasks). Additionally, FP8‚Äôs impact on out-of-distribution generalization is also unexplored.\nThe proposed global minimum scaling factor synchronization is efficient but introduces additional computation. Synchronizing scaling factors across GPUs could become a bottleneck at extreme scales (e.g., trillion-parameter models), but then again, there may be other bottle necks before this is ever an issue since the scaling factor grows logarithmically to the number of parameters as it scales via the tensors that carry the weights.\nThe FP8 benefits are highly hardware-dependent, and their benefits may not transfer well to older GPUs (e.g., A100), or may require specific fine-tuning of the processing unit to benefit from this implementation.\nThere is a lack of comparative analysis with other FP8 approaches. There exist alternative FP8 training methods beyond Nvidia TE; research such as Graphcore‚Äôs FP8 work or other mixed-precision techniques (e.g., hybrid 8-bit/16-bit schemes) could provide better baselines for comparison in their results as Nvidia TE only operates at the matrix multiplication level (linear forward-pass layer) rather than at the optimizer and syncronization layers.\nFP8 impact on inference remains unexplored. This paper primarly focuses on training efficiency without really touching on any computation or memory advantages at inference. Since FP8 training reduces memory footprints and computational overhead, it is worth investigating whether it also enables better model compression or quantization at inference time as well. This may provide insights into its viability for real-world deployment, particularly in resource-constrained environments.*\nFinally, there is no detailed analysis of quantization artifacts. While automatic scaling mitigates quantization errors, the paper does not provide a deep analysis of gradient statistics under FP8. This raises questions like ‚ÄúDo certain layers (e.g., attention vs. feedforward) suffer more from FP8 quantization?‚Äù and ‚ÄúWhat is the effect of FP8 on weight sparsity or activation distributions?‚Äù\nFinal Thoughts This paper was very thorough in its approach, addressing many different aspects of the underlying processes that allow for the training of neural networks. The approach affords a more efficient approach to training models without dramatically increasing complexity. The solution also seems to scale as the LLMs grow larger in their parameter count, which leads me to believe this is a ‚Äúlow-hanging fruit‚Äù implementation to improve training of future models, as well as generalizes to a variety of training paradigms. This paper opens the door to more open-source research-focused LLMs which may push out understanding of knowledge representation within vectors as well as accessibility to larger models. Additionally it opens the door for more optimized FP8 GPUs and iterations that allow for true observations in the compute and cost optimization FP8 mixed-precision implementations provide.\nOverall, FP8-LM is a significant step toward making LLM training more efficient. The proposed full-stack FP8 training framework unlocks substantial memory, communication, and compute savings, making scaling up next-gen foundation models more feasible.\nHowever, questions remain about robustness across different architectures, long-context tasks, and alternative quantization strategies. Future research should explore broader applications of FP8 and hybrid precision methods to further optimize LLM training.\nAreas for Further Exploration\nExtending to Other Architectures Future work should evaluate FP8 on different architectures beyond transformers (e.g., diffusion models, CNNs, MoE models). Applying FP8 to vision or multimodal models could reveal new insights. Long-Context and Novel Training Analysis Evaluating FP8 models on long-context benchmarks (e.g., LongBench, Needle-in-a-Haystack) would help determine if FP8 impacts context retention. Evaluating FP8 models on out-of-distribution data may also determine the effectiveness of the generalizability and capacities of these models in comparison to their higher-precision counterparts. Inference Considerations Investigating if FP8 models allows for better model compression or quantization at inference as well. ",
  "wordCount" : "2896",
  "inLanguage": "en",
  "datePublished": "2025-02-09T21:08:00Z",
  "dateModified": "2025-02-09T21:08:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://senankassem.com/article-reviews/25-02-09-fp8-lm-training-fp8-large-language-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Senan Adonis Kassem",
    "logo": {
      "@type": "ImageObject",
      "url": "https://senankassem.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://senankassem.com/" accesskey="h" title="Senan Adonis Kassem (Alt + H)">Senan Adonis Kassem</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://senankassem.com/" title="home">
                    <span>home</span>
                </a>
            </li>
            <li>
                <a href="https://senankassem.com/about/" title="about me">
                    <span>about me</span>
                </a>
            </li>
            <li>
                <a href="https://senankassem.com/article-reviews/" title="article reviews">
                    <span>article reviews</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      FP8-LM - Training FP8 Large Language Models
    </h1>
    <div class="post-meta"><span title='2025-02-09 21:08:00 +0000 UTC'>February 9, 2025</span>&nbsp;¬∑&nbsp;14 min

<hr>
      <p>üìñ <strong>DOI:</strong> <a href="https://arxiv.org/abs/2310.18313" target="_blank">https://arxiv.org/abs/2310.18313</a></p><hr>
      <p>üìÇ <strong>PDF:</strong> <a href="https://arxiv.org/pdf/2310.18313" target="_blank">Download Paper</a></p>
    </div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#key-contributions--insights" aria-label="Key Contributions &amp; Insights">Key Contributions &amp; Insights</a><ul>
                        
                <li>
                    <a href="#automatic-scaling" aria-label="Automatic Scaling">Automatic Scaling</a></li>
                <li>
                    <a href="#precision-decoupling" aria-label="Precision Decoupling">Precision Decoupling</a></li>
                <li>
                    <a href="#adapting-to-fp8-to-different-distributed-parallel-training" aria-label="Adapting to FP8 to Different Distributed Parallel Training">Adapting to FP8 to Different Distributed Parallel Training</a></li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li></ul>
                </li>
                <li>
                    <a href="#evaluation-of-the-paper" aria-label="Evaluation of the Paper">Evaluation of the Paper</a></li>
                <li>
                    <a href="#final-thoughts" aria-label="Final Thoughts">Final Thoughts</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Training Large Language Models (LLMs) is slow and costly; they are computationally expensive, requiring a lot of compute power, memory allocation, and time to effectively train. The current paradigm of training LLMs uses 16-bit number representations (BF16), 32-bit number representations (FP32), or a mixture of both.</p>
<p>The reason BF16 has been used as opposed to FP16 is because BF16 offers a wider range of potential values at the expense of precision, due to its exponent occupying more of the bits rather than the mantissa, which prevents underflow and overflow in comparison to FP16. This leads to better numerical stability while matching the performance of the full-precision FP32 using half the number of bits leading to a reduction in its memory footprints, improving computational efficiency and reducing communication overhead.</p>
<p>So although higher-precision number representations have the capability of representing numbers more precisely at the expense of extra space in memory and higher operational costs for mathematical operations (this may be worth the tradeoff in particular domains of scientific research, engineering simulations, and financial modeling where small uncertainties may compound leading to inaccurate calculations and therefore inaccurate conclusions), in the domain of deep learning the computational overhead is high. This means models must find a balance between the accuracy of high-precision floating point representations of numbers, and lower-precision representations that speed up and increase the efficiency of computations.</p>
<p>The current paradigm of deep learning emphasises increasing compute power to increase computational speeds of higher-precision numbers at the expense of increased compute costs. Additionally, it almost mandates that to improve the predictive capacity of neural networks, the only step forward is more data and compute power. This has set a precedent of maintaining higher-precision representations within hardware without offering true lower-precision representational capacities i.e. even if the GPUs/TPUs/CPUs offer FP8 support, their fundamental architectures are optimised for BF16, offering only minor perceived improvements in computational efficiency and speed when using FP8.</p>
<p>This paper theorizes that using a lower-precision number representation may lead to reductions in cost because they have lower communication overheads, faster numerical operations, and smaller memory footprints without significant deteriorations in overall predictive accuracy, which challenges the current approach of needing increased compute power for progression in the field of deep learning. It also argues that this change in number representation does not require changes to hyperparameters during training to maintain accuracy, signifying a significant (yet simple) improvement in the efficiency of training deep neural networks. And finally, it argues that these improvements are not linear, but rather scale multiplicatively as the size of the neural architecture (which has an exponentially increasing number of weights).</p>
<hr>
<h2 id="key-contributions--insights">Key Contributions &amp; Insights<a hidden class="anchor" aria-hidden="true" href="#key-contributions--insights">#</a></h2>
<p>This paper defines a novel approach FP8 mixed-precision training framework that incorporates 8-bit weights and gradients, 8-bit optimizers, and 8-bit distributed parallel training to allow for faster and more memory-efficient training while maintaining predictive accuracy of the models. This implementation does not require changes to hyper-parameters and training receipts. It addresses issues such as data underflow or overflow and quantization errors, which cause numerical instability and irreversible divergences throughout training, by using two techniques <em><strong>precision decoupling</strong></em> and <em><strong>automatic scaling</strong></em>. By using FP8 instead of BF16, they have noted a 39% reduction in real memory usage and 75% faster training.</p>
<h3 id="automatic-scaling"><strong>Automatic Scaling</strong><a hidden class="anchor" aria-hidden="true" href="#automatic-scaling">#</a></h3>
<p>Training LLMs with reduced-precision FP8 has its challenges; data overflow and underflow are much more problematic as the dynamic range and representation precision of FP8 is much lower than that of FP16 and BF16, which leads to training collapses caused by loss spikes (or even NaNs) and vanishing gradients. To address this, <em>tensor scaling techniques</em> are proposed - this is the idea where we multiply higher precision values with a scaling factor before converting them to FP8 in order to preserve the number in a range that corresponds with the representable range of FP8. This scaling factor is determined dynamically by checking the running statistics of the gradients and weights within the layers. This allows for higher-precision numbers to preserve their gradient values within the representation range when converted to FP8, alleviating underflow and overflow occurrences.</p>
<p><em><strong>FP8 Gradient and All-Reduce Communication</strong></em></p>
<p>The typical method of storing gradients for computation is done in 16/32-bit datatypes, which results in high bandwidth requirements for collective communication during training. This is when parameters (such as gradients, model weights, and activations) need to be exchanged <em>between</em> GPUs, usually through some collective communication operation such as AllReduce (summing gradients across multiple GPUs), Broadcasting (sending updated weights to all GPUs), and AllGather (Sharing activations across GPUs for specific architectures). The higher the size of the datatype - i.e. the number of bits it takes in memory - the more bytes are required to be communicated for each number representation. For example, if a model had 100 parameters, storing and communicating in FP32 would require 4*100 bytes, while FP16/BF16 cuts this cost in half.</p>
<p>As a model size increases, the number of parameters may grow exponentially, leading to exponentially increasing communication overhead. Additionally, communication overhead also depends on the number of GPUs, which means that although GPUs help distribute computation, it also increases synchronization costs between the GPUs. These combined may lead to a non-trivial growing bottleneck during training as the complexity of the model increases, and the number of parallel batches are processed.</p>
<p>Directly applying FP8 to gradients leads to a decrease in accuracy because of the underflow and overflow problems arising from the low-bit AllReduce operation. The AllReduce operation typically aggregates gradients across GPUs using <em>pre-scaling</em> (dividing gradients before summation) and <em>post-scaling</em> (dividing after summation) within a batch of inputs, where the goal is to average the gradient tensor at a given $ i $th layer, denoted as $ g_i $, across different GPUs. But using the <em>pre-scaling</em> and <em>post-scaling</em> leads to an issue of underflowing and overflowing respectively, so this paper proposes <em>automatic scaling</em> as a method to address this issue.</p>
<p>Additionally, FP8 needs per-tensor scaling factors, but current GPU communication frameworks do not efficiently support reducing them across GPUs. Therefore, the scalar that is defined is calculated as a single global scaler that is shared across all GPUs, which ensures that all gradient tensors for a given $i$th layer, $g_i$, use the same shared scaling factor when quantized into FP8 format. This approach significantly reduces communication overhead by limiting the number of scalers transmitted, making synchronization steps highly efficient and allowing low-bit gradient communication without extra complexity</p>
<h3 id="precision-decoupling"><strong>Precision Decoupling</strong><a hidden class="anchor" aria-hidden="true" href="#precision-decoupling">#</a></h3>
<p>It has been shown that reducing the precision of an optimisers variables leads to accuracy degradation, which raises the question: which variables in the optimizer must be kept at high precision? This is where precision decoupling comes, where the goal is to decouple the influence of data precision on the variables in the optimizer and investigate which one can be assigned lower precision.</p>
<p><em><strong>FP8 Optimizers</strong></em></p>
<p>Traditional optimizers, such as Adam, maintain copies of their model weights, gradients, first-order and second-order gradient moments in 32-bit float format for numerical stability. This leads to large 16byte overhead per parameter during training. To try and reduce this, a mixed-precision approach is taken: Gradient statistics can use lower precision, while the master weights necessitate high precision; direction of the gradient holds greater significance than its magnitude.</p>
<ul>
<li>Master weights need higher precision because during optimization weight updates can become extremely small or large making it harder to decipher the correct direction at low precision; the higher the precision helps prevent loss of information when updating weights, ensuring stable and accurate training in the correct direction.</li>
<li>The first- and second-order gradient moments are used more to scale magnitude than direction so do not require as high precision
<ul>
<li>The first-order gradient moment can tolerate a high quantization error and can be assigned with low-precision, albeit with some degradation.</li>
<li>The second-order moment requires a higher precision than the first-order gradient moment because calculating the square of gradients for the second-order gradient moment might lead to data underflow due to the typically small gradient values.</li>
</ul>
</li>
</ul>
<h3 id="adapting-to-fp8-to-different-distributed-parallel-training">Adapting to FP8 to Different Distributed Parallel Training<a hidden class="anchor" aria-hidden="true" href="#adapting-to-fp8-to-different-distributed-parallel-training">#</a></h3>
<p>Distributed strategies are necessary for parallelizing training across multiple GPUs and are often used in a complementary way to increase parallelism and efficient training. These methods include</p>
<ul>
<li>
<p><em>Data parallelism</em>: Distributing the same model across multiple devices, each processing a different subset of the data simultaneously.</p>
<ul>
<li>Since this approach doesn&rsquo;t involve splitting individual tensors or layers, integrating FP8 precision doesn&rsquo;t require specific modifications.</li>
</ul>
</li>
<li>
<p><em>Pipeline parallelism</em>: Dividing the model into sequential stages, each assigned to a different device, allowing different batches of data to be processed concurrently through the pipeline.</p>
<ul>
<li>Similar to data parallelism, this method doesn&rsquo;t necessitate additional FP8-specific adjustments because it doesn&rsquo;t involve partitioning tensors within layers and independently allocates distinct layers to different portions of the GPU.</li>
<li>Although it does not need to directly find a method to implement FP8, it is essential to ensure data integrity and congruency between the layers</li>
</ul>
</li>
<li>
<p><em>Tensor parallelism</em>: Splitting individual tensors (such as weight matrices) across multiple devices to perform computations like matrix multiplications in parallel.</p>
<ul>
<li>This type of distribution requires the sharded weights and activation tensors to be converted into FP8 format for computations in linear layers.</li>
<li>This enables both forward computations and backward gradient communications to utilize FP8 precision, enhancing efficiency.</li>
</ul>
</li>
<li>
<p><em>Sequence parallelism</em>: Distribute subsequences of the initial input into a transformer models across multiple devices, enabling parallel processing of different parts of the input sequence.</p>
<ul>
<li>This type of distribution requires a converter, $g$, to manage the transition between sequence and tensor parallel regions.</li>
<li>During the forward pass, an all-gather operation collects sequence partitions, while in the backward pass, a reduce-scatter operation handles tensor segments.</li>
<li>Incorporating FP8 datatype conversion before these operations reduces communication costs across GPUs by transmitting low-bit FP8 activations.</li>
</ul>
</li>
<li>
<p><em>Zero Redundancy Optimizer (ZeRO)</em>: Memory optimization technique that builds on data parallelism</p>
<ul>
<li>Partitions the model states (parameters, gradients, and optimizer states) across multiple devices reducing memory redundancies</li>
<li>Applying FP8 to this method is difficult because of managing scaling factors associated with FP8</li>
<li>Solution: Allocate each tensor as a whole across devices rather than partitioning into sub-tensors. This ensures that tensor scaling factors are distributed along with the tensor and reduces communication and computational complexity.</li>
</ul>
</li>
</ul>
<h3 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h3>
<p>Using FP8 mixed-precision did not lead to significant differences in the loss curves and demonstrate that the proposed FP8 mixed-precision scheme can achieve equivalent performance to those of its higher-precision counterparts across a diverse array of model scales, additionally there is comparable zero-short performance in comparison to downstream tasks in comparison to BF16 model counterparts. Furthermore, the communication costs and memory footprints dramatically decreased in comparison to its higher-precision counter parts, which further validates that models pre-trained with FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities comparable to those of higher-precision while lowering computational cost, communication overhead and reducing memory footprint.</p>
<p>These results also extrapolate to different approaches to LLMs, such as fine-tuning and reinforcement learning from human feedback, leading to dramatic decreases in memory utilization in the RL scenario (32% concerning model weights and 62% concerning optimizer states). This further reinforces the versatility and innovative solution FP8 has in advancing and improving the training process of LLMs while maintaining predictive quality.</p>
<hr>
<h2 id="evaluation-of-the-paper">Evaluation of the Paper<a hidden class="anchor" aria-hidden="true" href="#evaluation-of-the-paper">#</a></h2>
<p>This paper introduced a groudbreaking, paradigm shifting FP8 mixed-precision training framework that significantly improves memory efficieny and training speeds without significant sacrifices in performance. It provides a comprehensive open-source implementation of FP8 accross a variety of training components - gradients, optimizers, distributed communication, and parallel training strategies - as well as provides performance benchmarks in a variety of different training paradigms - pretraining, fine-tuning and even reinforcement learning with human feedback tasks - differentiating itself from prior approaches and providing a general solution to the field of LLM&rsquo;s as a whole.</p>
<p>With the results from this paper, it offers a way to make highly scalable solution for multi-GPU distributed training and opens the door to training even larger models at a lower computational cost and a faster pace. Its numerical stability methods of using automatic scaling rather than pre- and post-scaling ensures that FP8 weights and gradients are effectively trained through their corresponding optimizers at scale, and the decoupling of precision to ensure as much of the parameters are represented in FP8 increasing efficiency while preventing significant degradations in the model&rsquo;s performance.</p>
<p><em>These results alone are paradigm-shifting.</em></p>
<p>Additionally, the paper has done all this with the current limitations of GPU communication and optimization for FP8 operations, demonstrating that these results are yet to see their peak advancements in the domain of training and optimizing a transformer based architectures.</p>
<p>The paper has also led to the more recent explosion of the DeepSeek model that utilizes FP8 mixed-precision in its training (along with some other innovative approaches, <a href="https://senankassem.com/posts/25-02-11-deepSeek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning/" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">see DeepSeek-R1</a>) in building a competent model with Chain-of-Thought (CoT) reasoning at a fraction of the cost of current large-scale LLMs. It provides the groundwork for advancements in the field and opens up the door for future research related to the field of effective computation and communication within and between GPUs, the effects of scaling up FP8 models <a href="https://openreview.net/forum?id=E1EHO0imOb&amp;utm_source=chatgpt.com" title="Scaling to a Trillion Tokens">(example)</a>, as well as in regards to our understanding of knowledge representations and updating the vector embeddings.</p>
<p>Now comes some more critical analysis of this ground-breaking paper:</p>
<p><em>FP8 Training may not be as robust for all architectures</em>. The paper only focuses on GPT-style models (7B to 175B parameters); Different architectures, such as diffusion models, may exhibit different numerical stability issues with FP8 and the methods may not generalize well to non-transformer architectures. However, Mixture-of-Experts (MoE) - implemented by DeepSeek&rsquo;s R1 model - has been shown to work with this FP8 mixed-precision training paradigm, so there may be more generalizability than less.</p>
<p><em>There is a lack of evaluation on long-context or novel tasks</em>. While FP8 achieves comparable pretraining loss and zero-shot performance, there is no detailed analysis of fine-grained accuracy trade-offs in tasks that require long-term dependencies (e.g., long-context reasoning tasks). Additionally, FP8‚Äôs impact on out-of-distribution generalization is also unexplored.</p>
<p><em>The proposed global minimum scaling factor synchronization is efficient but introduces additional computation</em>. Synchronizing scaling factors across GPUs could become a bottleneck at extreme scales (e.g., trillion-parameter models), but then again, there may be other bottle necks before this is ever an issue since the scaling factor grows logarithmically to the number of parameters as it scales via the tensors that carry the weights.</p>
<p><em>The FP8 benefits are highly hardware-dependent</em>, and their benefits may not transfer well to older GPUs (e.g., A100), or may require specific fine-tuning of the processing unit to benefit from this implementation.</p>
<p>There is a <em>lack of comparative analysis with other FP8 approaches</em>. There exist alternative FP8 training methods beyond Nvidia TE; research such as Graphcore&rsquo;s FP8 work or other mixed-precision techniques (e.g., hybrid 8-bit/16-bit schemes) could provide better baselines for comparison in their results as Nvidia TE only operates at the matrix multiplication level (linear forward-pass layer) rather than at the optimizer and syncronization layers.</p>
<p><em>FP8 impact on inference remains unexplored</em>. This paper primarly focuses on training efficiency without really touching on any computation or memory advantages at inference. Since FP8 training reduces memory footprints and computational overhead, it is worth investigating whether it also enables better model compression or quantization at inference time as well. This may provide insights into its viability for real-world deployment, particularly in resource-constrained environments.*</p>
<p>Finally, there is <em>no detailed analysis of quantization artifacts</em>. While automatic scaling mitigates quantization errors, the paper does not provide a deep analysis of gradient statistics under FP8. This raises questions like &ldquo;Do certain layers (e.g., attention vs. feedforward) suffer more from FP8 quantization?&rdquo; and &ldquo;What is the effect of FP8 on weight sparsity or activation distributions?&rdquo;</p>
<hr>
<h2 id="final-thoughts">Final Thoughts<a hidden class="anchor" aria-hidden="true" href="#final-thoughts">#</a></h2>
<p>This paper was very thorough in its approach, addressing many different aspects of the underlying processes that allow for the training of neural networks. The approach affords a more efficient approach to training models without dramatically increasing complexity. The solution also seems to scale as the LLMs grow larger in their parameter count, which leads me to believe this is a &ldquo;low-hanging fruit&rdquo; implementation to improve training of future models, as well as generalizes to a variety of training paradigms. This paper opens the door to more open-source research-focused LLMs which may push out understanding of knowledge representation within vectors as well as accessibility to larger models. Additionally it opens the door for more optimized FP8 GPUs and iterations that allow for true observations in the compute and cost optimization FP8 mixed-precision implementations provide.</p>
<p>Overall, FP8-LM is a significant step toward making LLM training more efficient. The proposed full-stack FP8 training framework unlocks substantial memory, communication, and compute savings, making scaling up next-gen foundation models more feasible.</p>
<p>However, questions remain about robustness across different architectures, long-context tasks, and alternative quantization strategies. Future research should explore broader applications of FP8 and hybrid precision methods to further optimize LLM training.</p>
<p>Areas for Further Exploration</p>
<ol>
<li>Extending to Other Architectures</li>
</ol>
<ul>
<li>Future work should evaluate FP8 on different architectures beyond transformers (e.g., diffusion models, CNNs, MoE models).</li>
<li>Applying FP8 to vision or multimodal models could reveal new insights.</li>
</ul>
<ol start="2">
<li>Long-Context and Novel Training Analysis</li>
</ol>
<ul>
<li>Evaluating FP8 models on long-context benchmarks (e.g., LongBench, Needle-in-a-Haystack) would help determine if FP8 impacts context retention.</li>
<li>Evaluating FP8 models on out-of-distribution data may also determine the effectiveness of the generalizability and capacities of these models in comparison to their higher-precision counterparts.</li>
</ul>
<ol start="3">
<li>Inference Considerations</li>
</ol>
<ul>
<li>Investigating if FP8 models allows for better model compression or quantization at inference as well.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share FP8-LM - Training FP8 Large Language Models on x"
            href="https://x.com/intent/tweet/?text=FP8-LM%20-%20Training%20FP8%20Large%20Language%20Models&amp;url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f&amp;hashtags=NLP%2cTransformers%2cDeepLearning%2cLLM%27s">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share FP8-LM - Training FP8 Large Language Models on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f&amp;title=FP8-LM%20-%20Training%20FP8%20Large%20Language%20Models&amp;summary=FP8-LM%20-%20Training%20FP8%20Large%20Language%20Models&amp;source=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share FP8-LM - Training FP8 Large Language Models on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f&title=FP8-LM%20-%20Training%20FP8%20Large%20Language%20Models">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share FP8-LM - Training FP8 Large Language Models on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share FP8-LM - Training FP8 Large Language Models on whatsapp"
            href="https://api.whatsapp.com/send?text=FP8-LM%20-%20Training%20FP8%20Large%20Language%20Models%20-%20https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share FP8-LM - Training FP8 Large Language Models on telegram"
            href="https://telegram.me/share/url?text=FP8-LM%20-%20Training%20FP8%20Large%20Language%20Models&amp;url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share FP8-LM - Training FP8 Large Language Models on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=FP8-LM%20-%20Training%20FP8%20Large%20Language%20Models&u=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-09-fp8-lm-training-fp8-large-language-models%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://senankassem.com/">Senan Adonis Kassem</a></span> ¬∑ 


</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
