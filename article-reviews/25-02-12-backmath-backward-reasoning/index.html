<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step | Senan Adonis Kassem</title>
<meta name="keywords" content="NLP, Transformers, Deep Learning, Chain of Thought, Reinforcement Learning, Supervised Training/Fine-Tuning">
<meta name="description" content="A review of the BackMATH paper, covering its impact on training models for reasoning, key insights, and future directions.">
<meta name="author" content="">
<link rel="canonical" href="https://senankassem.com/article-reviews/25-02-12-backmath-backward-reasoning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f29b4ec3ee6e98a9ec1a9a51adb5b1bf468ca1f94fcea7e42282ee958b7e23e5.css" integrity="sha256-8ptOw&#43;5umKnsGppRrbWxv0aMoflPzqfkIoLulYt&#43;I&#43;U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://senankassem.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://senankassem.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://senankassem.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://senankassem.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://senankassem.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://senankassem.com/article-reviews/25-02-12-backmath-backward-reasoning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://senankassem.com/article-reviews/25-02-12-backmath-backward-reasoning/">
  <meta property="og:site_name" content="Senan Adonis Kassem">
  <meta property="og:title" content="BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step">
  <meta property="og:description" content="A review of the BackMATH paper, covering its impact on training models for reasoning, key insights, and future directions.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="article-reviews">
    <meta property="article:published_time" content="2025-02-12T15:10:31+00:00">
    <meta property="article:modified_time" content="2025-02-12T15:10:31+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step">
<meta name="twitter:description" content="A review of the BackMATH paper, covering its impact on training models for reasoning, key insights, and future directions.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Article Reviews",
      "item": "https://senankassem.com/article-reviews/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step",
      "item": "https://senankassem.com/article-reviews/25-02-12-backmath-backward-reasoning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step",
  "name": "BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step",
  "description": "A review of the BackMATH paper, covering its impact on training models for reasoning, key insights, and future directions.",
  "keywords": [
    "NLP", "Transformers", "Deep Learning", "Chain of Thought", "Reinforcement Learning", "Supervised Training/Fine-Tuning"
  ],
  "articleBody": "Introduction This paper is trying to address two things:\nLLM’s are bad at reasoning (specifically in complex problems). When they are able to reason, they cannot extrapolate to situations that differ from those it was trained on To address this, they gather a variety of math problems to start fine-tuning a model using Supervised Fine-Tuning (SFT) to enforce Chain-of-Thought (CoT) reasoning, as well as finetune the model to reverse problems using backward reasoning, where the final answer becomes part of the conditions, and the model figures out missing information. Once SFT has been completed, the model is then trained using a Reinforcement Learning (RL) paradigm using two reward models (PRM and BackPRM) to score the quality of the forward and backward reasoning steps, which is enforced using Proximal Policy Optimization (PPO), which refines the model based on feedback from the reward models defined.\nThis goal is to enforce both chain-of-thought reasoning, and the ability to restructure conditions and missing information. By doing so, they create a model that is capable of engaging with a complex problem through CoT reasoning, as well as adapt its ability to reason to more novel situations where the specific information that is missing and its conditions are not the same as examples it has been fine-tuned on i.e. it should in theory become more flexible in it’s problem-solving capabilities without relying strictly on memorization of CoT patterns.\nKey Contributions \u0026 Insights The results show that using SFT and RL produces models that are more capable at answering questions from a given testset of math questions than existing models. This may be caused by an increase in the adapability and flexibility of models that are trained using this paradigm in answering complex problems and engaging in efficient and structured problem solving.\nHowever, DeepSeek’s R1 model has a similar approach of RL, however instead of explicitly training the model with CoT examples and structured backward reasoning, DeepSeek directly uses RL to encourage step-by-step reasoning as an emergent behavior. This means that CoT reasoning naturally appears as a result of optimizing the reward function, rather than being explicitly trained through labeled CoT examples, which may lead to CoT reasoning being more deeply embedded within the weights of the model, and therefore be more likely to engage with CoT reasoning without being prompted to do so i.e. instead of requiring CoT-style inputs, the model may naturally adopt CoT reasoning across a broader range of problems as a defacto approach. Additionally, if a model implicitly learns structured reasoning patterns, it may be more likely to discover novel reasoning strategies on its own, some of which may be more effective than the ones we already know!\nAlthough DeepSeek’s approach may provide more general problem solving, the understanding of specific problems and concepts may be better captured via SFT and RL of the forward and backward problems as it provides direct examples of bidirectional learning and may encourage problem formulation to occur in a more systematic way. This is something that has been seen in DeepSeek where the CoT reasoning may not initially be obvious to the user/be foreign in its approach (non-readable to a human), yet the answer is correct.\nEvaluation of the Paper One of the issues with this paper is that there is a heavy reliance and dependence on manually curated backward reasoning data. This limits scalability and requires a human in the loop, which raises questions of accuracy/human-error within the dataset.\nIn comparison to DeepSeek’s R1 model, BackMATH uses PPO, which is more computationally expensive than GPRO used by DeepSeek, which raises questions about how adoptable and scalable its approach is in comparison to DeepSeek’s research.\nThe paper also assumes that training on backward reasoning helps models generalize to novel problems, which may not be fully true, and has not yet been fully proven. The model only improves by 1.6% on GSM8K and 2.1% on MATH, which are not significant in regards their comparison to competitor’s models. Additionally, real-world problems are often less defined and require an additional step of problem formulation for accurate complex problem solving, which the SFT examples lack. A broader evaluation of its problem-solving capabilities is required before proving this approach’s effectiveness in generalizaing to larger/differing domains beyond the training data.\nFinally, this paper primarily highlights successes but does not deeply analyze when and why the model still fails. So although, back-reasoning is a novel approach in its attempts to produce more effective reasoning, its lacks transparency/in-depth analysis of where the model may fall short and therefore does not explore when backward reasoning is not helpful for a model.\nFinal Thoughts BackMATH is well-motivated and approach to improving mathematical reasoning. It demonstrates that backward reasoning training can enhance problem-solving flexibility. However, it has scalability issues, computational inefficiencies, and uncertain generalization benefits. Future research should explore hybrid approaches combining self-supervised learning of structured reasoning training with emergent learning to build LLMs that are both interpretable and highly generalizable.\nAdditionally, the comparison between DeepSeek’s approach and BackMATH’s approach highlights potentially differing approaches to reasoning and rationality. It also raises the following questions:\nLanguages, Reasoning and Rationality Are languages that we use limiting our current approach to reasoning and rationality Could AI models develop more efficient or structured reasoning methods that differ from human linguistic reasoning? Would non-verbal or symbolic reasoning be a more optimal approach for AI reasoning? Will there be a new emergent language for reasoning that develop? Could neural networks evolve their own internal structures for logic and deduction, distinct from human cognition? Would such a system resemble mathematical logic, symbolic reasoning, or a novel form of abstract representation? Trust and Interpretability of Opaque AI Reasoning How can we trust opaque reasoning processes that produce correct outputs but are not interpretable? If a model like DeepSeek reasons in an unknown or non-human-readable way, how do we validate its correctness? Does the emergence of “black-box” AI reasoning present a fundamental limit to AI alignment? How to illuminate the opaque reasoning processes What interpretability methods (e.g., activation visualization, attribution analysis, mechanistic interpretability) can be applied? Are there patterns in AI-generated reasoning that can be reverse-engineered into human-understandable logic? Ethical Fine-Tuning of AI Reasoning How to safely fine-tune these processes to remain ethical will maintaining its efficacy What safeguards are needed to prevent emergent reasoning from diverging from human moral frameworks? Would we use Reinforcement Learning to develop ethics that align with human ethics Can we use Supervised Training/Fine-tuning to achieve the same results? Knowledge Representation in AI Do the weights of DeepSeek provide novel insights into our understanding of knowledge representations Could analyzing DeepSeek’s internal representations help us understand how AI models encode knowledge? Do AI systems create novel cognitive structures that challenge our existing understanding of epistemology and knowledge storage? ",
  "wordCount" : "1125",
  "inLanguage": "en",
  "datePublished": "2025-02-12T15:10:31Z",
  "dateModified": "2025-02-12T15:10:31Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://senankassem.com/article-reviews/25-02-12-backmath-backward-reasoning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Senan Adonis Kassem",
    "logo": {
      "@type": "ImageObject",
      "url": "https://senankassem.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://senankassem.com/" accesskey="h" title="Senan Adonis Kassem (Alt + H)">Senan Adonis Kassem</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://senankassem.com/" title="home">
                    <span>home</span>
                </a>
            </li>
            <li>
                <a href="https://senankassem.com/about/" title="about me">
                    <span>about me</span>
                </a>
            </li>
            <li>
                <a href="https://senankassem.com/article-reviews/" title="article reviews">
                    <span>article reviews</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step
    </h1>
    <div class="post-meta"><span title='2025-02-12 15:10:31 +0000 UTC'>February 12, 2025</span>&nbsp;·&nbsp;6 min

<hr>
      <p>📖 <strong>DOI:</strong> <a href="https://aclanthology.org/2025.coling-industry.40/" target="_blank">https://aclanthology.org/2025.coling-industry.40/</a></p><hr>
      <p>📂 <strong>PDF:</strong> <a href="https://aclanthology.org/2025.coling-industry.40.pdf" target="_blank">Download Paper</a></p>
    </div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#key-contributions--insights" aria-label="Key Contributions &amp; Insights">Key Contributions &amp; Insights</a></li>
                <li>
                    <a href="#evaluation-of-the-paper" aria-label="Evaluation of the Paper">Evaluation of the Paper</a></li>
                <li>
                    <a href="#final-thoughts" aria-label="Final Thoughts">Final Thoughts</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>This paper is trying to address two things:</p>
<blockquote>
<ol>
<li>LLM&rsquo;s are bad at reasoning (specifically in complex problems).</li>
<li>When they are able to reason, they cannot extrapolate to situations that differ from those it was trained on</li>
</ol></blockquote>
<p>To address this, they gather a variety of math problems to start fine-tuning a model using Supervised Fine-Tuning (SFT) to enforce Chain-of-Thought (CoT) reasoning, as well as finetune the model to reverse problems using backward reasoning, where the final answer becomes part of the conditions, and the model figures out missing information. Once SFT has been completed, the model is then trained using a Reinforcement Learning (RL) paradigm using two reward models (PRM and BackPRM) to score the quality of the forward and backward reasoning steps, which is enforced using Proximal Policy Optimization (PPO), which refines the model based on feedback from the reward models defined.</p>
<p>This goal is to enforce both chain-of-thought reasoning, and the ability to restructure conditions and missing information. By doing so, they create a model that is capable of engaging with a complex problem through CoT reasoning, as well as adapt its ability to reason to more novel situations where the specific information that is missing and its conditions are not the same as examples it has been fine-tuned on i.e. it should in theory become more flexible in it&rsquo;s problem-solving capabilities without relying strictly on memorization of CoT patterns.</p>
<hr>
<h3 id="key-contributions--insights">Key Contributions &amp; Insights<a hidden class="anchor" aria-hidden="true" href="#key-contributions--insights">#</a></h3>
<blockquote>
<p>The results show that <strong>using SFT and RL produces models that are more capable at answering questions</strong> from a given testset of math questions than existing models. This may be caused by an <strong>increase in the adapability and flexibility of models</strong> that are trained using this paradigm in answering complex problems and engaging in efficient and structured problem solving.</p></blockquote>
<p>However, DeepSeek&rsquo;s R1 model has a similar approach of RL, however instead of explicitly training the model with CoT examples and structured backward reasoning, DeepSeek directly uses RL to encourage step-by-step reasoning as an emergent behavior. This means that CoT reasoning naturally appears as a result of optimizing the reward function, rather than being explicitly trained through labeled CoT examples, which may lead to CoT reasoning being more deeply embedded within the weights of the model, and therefore be more likely to engage with CoT reasoning without being prompted to do so i.e. instead of requiring CoT-style inputs, the model may naturally adopt CoT reasoning across a broader range of problems as a defacto approach. Additionally, if a model implicitly learns structured reasoning patterns, it may be more likely to discover novel reasoning strategies on its own, some of which may be more effective than the ones we already know!</p>
<p>Although DeepSeek&rsquo;s approach may provide more general problem solving, the understanding of specific problems and concepts may be better captured via SFT and RL of the forward and backward problems as it provides direct examples of bidirectional learning and may encourage problem formulation to occur in a more systematic way. This is something that has been seen in DeepSeek where the CoT reasoning may not initially be obvious to the user/be foreign in its approach (non-readable to a human), yet the answer is correct.</p>
<hr>
<h3 id="evaluation-of-the-paper">Evaluation of the Paper<a hidden class="anchor" aria-hidden="true" href="#evaluation-of-the-paper">#</a></h3>
<p>One of the issues with this paper is that there is a heavy reliance and dependence on manually curated backward reasoning data. This limits scalability and requires a human in the loop, which raises questions of accuracy/human-error within the dataset.</p>
<p>In comparison to DeepSeek&rsquo;s R1 model, BackMATH uses PPO, which is more computationally expensive than GPRO used by DeepSeek, which raises questions about how adoptable and scalable its approach is in comparison to DeepSeek&rsquo;s research.</p>
<p>The paper also assumes that training on backward reasoning helps models generalize to novel problems, which may not be fully true, and has not yet been fully proven. The model only improves by 1.6% on GSM8K and 2.1% on MATH, which are not significant in regards their comparison to competitor&rsquo;s models. Additionally, real-world problems are often less defined and require an additional step of problem formulation for accurate complex problem solving, which the SFT examples lack. A broader evaluation of its problem-solving capabilities is required before proving this approach&rsquo;s effectiveness in generalizaing to larger/differing domains beyond the training data.</p>
<p>Finally, this paper primarily highlights successes but does not deeply analyze when and why the model still fails. So although, back-reasoning is a novel approach in its attempts to produce more effective reasoning, its lacks transparency/in-depth analysis of where the model may fall short and therefore does not explore when backward reasoning is not helpful for a model.</p>
<hr>
<h3 id="final-thoughts">Final Thoughts<a hidden class="anchor" aria-hidden="true" href="#final-thoughts">#</a></h3>
<p><em>BackMATH</em> is well-motivated and approach to improving mathematical reasoning. It demonstrates that <em>backward reasoning training can enhance problem-solving flexibility</em>. However, it has <em>scalability issues, computational inefficiencies, and uncertain generalization benefits</em>. Future research should explore hybrid approaches combining self-supervised learning of structured reasoning training with emergent learning to build LLMs that are both interpretable and highly generalizable.</p>
<p>Additionally, the comparison between DeepSeek&rsquo;s approach and BackMATH&rsquo;s approach highlights potentially differing approaches to reasoning and rationality. It also raises the following questions:</p>
<ol>
<li><em><strong>Languages, Reasoning and Rationality</strong></em>
<ul>
<li>Are languages that we use limiting our current approach to reasoning and rationality
<ul>
<li>Could AI models develop more efficient or structured reasoning methods that differ from human linguistic reasoning?</li>
<li>Would non-verbal or symbolic reasoning be a more optimal approach for AI reasoning?</li>
</ul>
</li>
<li>Will there be a new emergent language for reasoning that develop?
<ul>
<li>Could neural networks evolve their own internal structures for logic and deduction, distinct from human cognition?</li>
<li>Would such a system resemble mathematical logic, symbolic reasoning, or a novel form of abstract representation?</li>
</ul>
</li>
</ul>
</li>
<li><em><strong>Trust and Interpretability of Opaque AI Reasoning</strong></em>
<ul>
<li>How can we trust opaque reasoning processes that produce correct outputs but are not interpretable?
<ul>
<li>If a model like DeepSeek reasons in an unknown or non-human-readable way, how do we validate its correctness?</li>
<li>Does the emergence of &ldquo;black-box&rdquo; AI reasoning present a fundamental limit to AI alignment?</li>
</ul>
</li>
<li>How to illuminate the opaque reasoning processes
<ul>
<li>What interpretability methods (e.g., activation visualization, attribution analysis, mechanistic interpretability) can be applied?</li>
<li>Are there patterns in AI-generated reasoning that can be reverse-engineered into human-understandable logic?</li>
</ul>
</li>
</ul>
</li>
<li><em><strong>Ethical Fine-Tuning of AI Reasoning</strong></em>
<ul>
<li>How to safely fine-tune these processes to remain ethical will maintaining its efficacy
<ul>
<li>What safeguards are needed to prevent emergent reasoning from diverging from human moral frameworks?</li>
<li>Would we use Reinforcement Learning to develop ethics that align with human ethics</li>
<li>Can we use Supervised Training/Fine-tuning to achieve the same results?</li>
</ul>
</li>
</ul>
</li>
<li><em><strong>Knowledge Representation in AI</strong></em>
<ul>
<li>Do the weights of DeepSeek provide novel insights into our understanding of knowledge representations
<ul>
<li>Could analyzing DeepSeek’s internal representations help us understand how AI models encode knowledge?</li>
<li>Do AI systems create novel cognitive structures that challenge our existing understanding of epistemology and knowledge storage?</li>
</ul>
</li>
</ul>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step on x"
            href="https://x.com/intent/tweet/?text=BackMATH%20-%20Towards%20Backward%20Reasoning%20for%20Solving%20Math%20Problems%20Step%20by%20Step&amp;url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f&amp;hashtags=NLP%2cTransformers%2cDeepLearning%2cChainofThought%2cReinforcementLearning%2cSupervisedTraining%2fFine-Tuning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f&amp;title=BackMATH%20-%20Towards%20Backward%20Reasoning%20for%20Solving%20Math%20Problems%20Step%20by%20Step&amp;summary=BackMATH%20-%20Towards%20Backward%20Reasoning%20for%20Solving%20Math%20Problems%20Step%20by%20Step&amp;source=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f&title=BackMATH%20-%20Towards%20Backward%20Reasoning%20for%20Solving%20Math%20Problems%20Step%20by%20Step">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step on whatsapp"
            href="https://api.whatsapp.com/send?text=BackMATH%20-%20Towards%20Backward%20Reasoning%20for%20Solving%20Math%20Problems%20Step%20by%20Step%20-%20https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step on telegram"
            href="https://telegram.me/share/url?text=BackMATH%20-%20Towards%20Backward%20Reasoning%20for%20Solving%20Math%20Problems%20Step%20by%20Step&amp;url=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share BackMATH - Towards Backward Reasoning for Solving Math Problems Step by Step on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=BackMATH%20-%20Towards%20Backward%20Reasoning%20for%20Solving%20Math%20Problems%20Step%20by%20Step&u=https%3a%2f%2fsenankassem.com%2farticle-reviews%2f25-02-12-backmath-backward-reasoning%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://senankassem.com/">Senan Adonis Kassem</a></span> · 


</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
